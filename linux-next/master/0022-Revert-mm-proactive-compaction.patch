From 7c564f462d90730ded3b8969ae875a1beb62c436 Mon Sep 17 00:00:00 2001
From: Nathan Chancellor <natechancellor@gmail.com>
Date: Mon, 22 Jun 2020 19:32:17 -0700
Subject: [PATCH 22/22] Revert "mm: proactive compaction"

This reverts commit a0e396e6f93caab4695fa13593b560f0a83abb8c.

Link: https://lore.kernel.org/lkml/20200623022636.GA1051134@ubuntu-n2-xlarge-x86/
Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
---
 Documentation/admin-guide/sysctl/vm.rst |  15 --
 include/linux/compaction.h              |   2 -
 kernel/sysctl.c                         |   9 --
 mm/compaction.c                         | 183 +-----------------------
 mm/internal.h                           |   1 -
 mm/vmstat.c                             |  18 ---
 6 files changed, 5 insertions(+), 223 deletions(-)

diff --git a/Documentation/admin-guide/sysctl/vm.rst b/Documentation/admin-guide/sysctl/vm.rst
index 4b7c496199ca..d46d5b7013c6 100644
--- a/Documentation/admin-guide/sysctl/vm.rst
+++ b/Documentation/admin-guide/sysctl/vm.rst
@@ -119,21 +119,6 @@ all zones are compacted such that free memory is available in contiguous
 blocks where possible. This can be important for example in the allocation of
 huge pages although processes will also directly compact memory as required.
 
-compaction_proactiveness
-========================
-
-This tunable takes a value in the range [0, 100] with a default value of
-20. This tunable determines how aggressively compaction is done in the
-background. Setting it to 0 disables proactive compaction.
-
-Note that compaction has a non-trivial system-wide impact as pages
-belonging to different processes are moved around, which could also lead
-to latency spikes in unsuspecting applications. The kernel employs
-various heuristics to avoid wasting CPU cycles if it detects that
-proactive compaction is not being effective.
-
-Be careful when setting it to extreme values like 100, as that may
-cause excessive background compaction activity.
 
 compact_unevictable_allowed
 ===========================
diff --git a/include/linux/compaction.h b/include/linux/compaction.h
index 7a242d46454e..6fa0eea3f530 100644
--- a/include/linux/compaction.h
+++ b/include/linux/compaction.h
@@ -85,13 +85,11 @@ static inline unsigned long compact_gap(unsigned int order)
 
 #ifdef CONFIG_COMPACTION
 extern int sysctl_compact_memory;
-extern int sysctl_compaction_proactiveness;
 extern int sysctl_compaction_handler(struct ctl_table *table, int write,
 			void *buffer, size_t *length, loff_t *ppos);
 extern int sysctl_extfrag_threshold;
 extern int sysctl_compact_unevictable_allowed;
 
-extern int extfrag_for_order(struct zone *zone, unsigned int order);
 extern int fragmentation_index(struct zone *zone, unsigned int order);
 extern enum compact_result try_to_compact_pages(gfp_t gfp_mask,
 		unsigned int order, unsigned int alloc_flags,
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index cac5a14a5184..4aea67d3d552 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -2844,15 +2844,6 @@ static struct ctl_table vm_table[] = {
 		.mode		= 0200,
 		.proc_handler	= sysctl_compaction_handler,
 	},
-	{
-		.procname	= "compaction_proactiveness",
-		.data		= &sysctl_compaction_proactiveness,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_minmax,
-		.extra1		= SYSCTL_ZERO,
-		.extra2		= &one_hundred,
-	},
 	{
 		.procname	= "extfrag_threshold",
 		.data		= &sysctl_extfrag_threshold,
diff --git a/mm/compaction.c b/mm/compaction.c
index 544a98811c82..86375605faa9 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -50,24 +50,6 @@ static inline void count_compact_events(enum vm_event_item item, long delta)
 #define pageblock_start_pfn(pfn)	block_start_pfn(pfn, pageblock_order)
 #define pageblock_end_pfn(pfn)		block_end_pfn(pfn, pageblock_order)
 
-/*
- * Fragmentation score check interval for proactive compaction purposes.
- */
-static const int HPAGE_FRAG_CHECK_INTERVAL_MSEC = 500;
-
-/*
- * Page order with-respect-to which proactive compaction
- * calculates external fragmentation, which is used as
- * the "fragmentation score" of a node/zone.
- */
-#if defined CONFIG_TRANSPARENT_HUGEPAGE
-#define COMPACTION_HPAGE_ORDER	HPAGE_PMD_ORDER
-#elif defined HUGETLB_PAGE_ORDER
-#define COMPACTION_HPAGE_ORDER	HUGETLB_PAGE_ORDER
-#else
-#define COMPACTION_HPAGE_ORDER	(PMD_SHIFT - PAGE_SHIFT)
-#endif
-
 static unsigned long release_freepages(struct list_head *freelist)
 {
 	struct page *page, *next;
@@ -1875,76 +1857,6 @@ static inline bool is_via_compact_memory(int order)
 	return order == -1;
 }
 
-static bool kswapd_is_running(pg_data_t *pgdat)
-{
-	return pgdat->kswapd && (pgdat->kswapd->state == TASK_RUNNING);
-}
-
-/*
- * A zone's fragmentation score is the external fragmentation wrt to the
- * COMPACTION_HPAGE_ORDER scaled by the zone's size. It returns a value
- * in the range [0, 100].
- *
- * The scaling factor ensures that proactive compaction focuses on larger
- * zones like ZONE_NORMAL, rather than smaller, specialized zones like
- * ZONE_DMA32. For smaller zones, the score value remains close to zero,
- * and thus never exceeds the high threshold for proactive compaction.
- */
-static int fragmentation_score_zone(struct zone *zone)
-{
-	unsigned long score;
-
-	score = zone->present_pages *
-			extfrag_for_order(zone, COMPACTION_HPAGE_ORDER);
-	return div64_ul(score, zone->zone_pgdat->node_present_pages + 1);
-}
-
-/*
- * The per-node proactive (background) compaction process is started by its
- * corresponding kcompactd thread when the node's fragmentation score
- * exceeds the high threshold. The compaction process remains active till
- * the node's score falls below the low threshold, or one of the back-off
- * conditions is met.
- */
-static int fragmentation_score_node(pg_data_t *pgdat)
-{
-	unsigned long score = 0;
-	int zoneid;
-
-	for (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {
-		struct zone *zone;
-
-		zone = &pgdat->node_zones[zoneid];
-		score += fragmentation_score_zone(zone);
-	}
-
-	return score;
-}
-
-static int fragmentation_score_wmark(pg_data_t *pgdat, bool low)
-{
-	int wmark_low;
-
-	/*
-	 * Cap the low watermak to avoid excessive compaction
-	 * activity in case a user sets the proactivess tunable
-	 * close to 100 (maximum).
-	 */
-	wmark_low = max(100 - sysctl_compaction_proactiveness, 5);
-	return low ? wmark_low : min(wmark_low + 10, 100);
-}
-
-static bool should_proactive_compact_node(pg_data_t *pgdat)
-{
-	int wmark_high;
-
-	if (!sysctl_compaction_proactiveness || kswapd_is_running(pgdat))
-		return false;
-
-	wmark_high = fragmentation_score_wmark(pgdat, false);
-	return fragmentation_score_node(pgdat) > wmark_high;
-}
-
 static enum compact_result __compact_finished(struct compact_control *cc)
 {
 	unsigned int order;
@@ -1971,25 +1883,6 @@ static enum compact_result __compact_finished(struct compact_control *cc)
 			return COMPACT_PARTIAL_SKIPPED;
 	}
 
-	if (cc->proactive_compaction) {
-		int score, wmark_low;
-		pg_data_t *pgdat;
-
-		pgdat = cc->zone->zone_pgdat;
-		if (kswapd_is_running(pgdat))
-			return COMPACT_PARTIAL_SKIPPED;
-
-		score = fragmentation_score_zone(cc->zone);
-		wmark_low = fragmentation_score_wmark(pgdat, true);
-
-		if (score > wmark_low)
-			ret = COMPACT_CONTINUE;
-		else
-			ret = COMPACT_SUCCESS;
-
-		goto out;
-	}
-
 	if (is_via_compact_memory(cc->order))
 		return COMPACT_CONTINUE;
 
@@ -2048,7 +1941,6 @@ static enum compact_result __compact_finished(struct compact_control *cc)
 		}
 	}
 
-out:
 	if (cc->contended || fatal_signal_pending(current))
 		ret = COMPACT_CONTENDED;
 
@@ -2529,41 +2421,6 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,
 	return rc;
 }
 
-/*
- * Compact all zones within a node till each zone's fragmentation score
- * reaches within proactive compaction thresholds (as determined by the
- * proactiveness tunable).
- *
- * It is possible that the function returns before reaching score targets
- * due to various back-off conditions, such as, contention on per-node or
- * per-zone locks.
- */
-static void proactive_compact_node(pg_data_t *pgdat)
-{
-	int zoneid;
-	struct zone *zone;
-	struct compact_control cc = {
-		.order = -1,
-		.mode = MIGRATE_SYNC_LIGHT,
-		.ignore_skip_hint = true,
-		.whole_zone = true,
-		.gfp_mask = GFP_KERNEL,
-		.proactive_compaction = true,
-	};
-
-	for (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {
-		zone = &pgdat->node_zones[zoneid];
-		if (!populated_zone(zone))
-			continue;
-
-		cc.zone = zone;
-
-		compact_zone(&cc, NULL);
-
-		VM_BUG_ON(!list_empty(&cc.freepages));
-		VM_BUG_ON(!list_empty(&cc.migratepages));
-	}
-}
 
 /* Compact all zones within a node */
 static void compact_node(int nid)
@@ -2610,13 +2467,6 @@ static void compact_nodes(void)
 /* The written value is actually unused, all memory is compacted */
 int sysctl_compact_memory;
 
-/*
- * Tunable for proactive compaction. It determines how
- * aggressively the kernel should compact memory in the
- * background. It takes values in the range [0, 100].
- */
-int __read_mostly sysctl_compaction_proactiveness = 20;
-
 /*
  * This is the entry point for compacting all nodes via
  * /proc/sys/vm/compact_memory
@@ -2796,7 +2646,6 @@ static int kcompactd(void *p)
 {
 	pg_data_t *pgdat = (pg_data_t*)p;
 	struct task_struct *tsk = current;
-	unsigned int proactive_defer = 0;
 
 	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);
 
@@ -2812,34 +2661,12 @@ static int kcompactd(void *p)
 		unsigned long pflags;
 
 		trace_mm_compaction_kcompactd_sleep(pgdat->node_id);
-		if (wait_event_freezable_timeout(pgdat->kcompactd_wait,
-			kcompactd_work_requested(pgdat),
-			msecs_to_jiffies(HPAGE_FRAG_CHECK_INTERVAL_MSEC))) {
-
-			psi_memstall_enter(&pflags);
-			kcompactd_do_work(pgdat);
-			psi_memstall_leave(&pflags);
-			continue;
-		}
+		wait_event_freezable(pgdat->kcompactd_wait,
+				kcompactd_work_requested(pgdat));
 
-		/* kcompactd wait timeout */
-		if (should_proactive_compact_node(pgdat)) {
-			unsigned int prev_score, score;
-
-			if (proactive_defer) {
-				proactive_defer--;
-				continue;
-			}
-			prev_score = fragmentation_score_node(pgdat);
-			proactive_compact_node(pgdat);
-			score = fragmentation_score_node(pgdat);
-			/*
-			 * Defer proactive compaction if the fragmentation
-			 * score did not go down i.e. no progress made.
-			 */
-			proactive_defer = score < prev_score ?
-					0 : 1 << COMPACT_MAX_DEFER_SHIFT;
-		}
+		psi_memstall_enter(&pflags);
+		kcompactd_do_work(pgdat);
+		psi_memstall_leave(&pflags);
 	}
 
 	return 0;
diff --git a/mm/internal.h b/mm/internal.h
index 42cf0b610847..9886db20d94f 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -239,7 +239,6 @@ struct compact_control {
 	bool no_set_skip_hint;		/* Don't mark blocks for skipping */
 	bool ignore_block_suitable;	/* Scan blocks considered unsuitable */
 	bool direct_compaction;		/* False from kcompactd or /proc/... */
-	bool proactive_compaction;	/* kcompactd proactive compaction */
 	bool whole_zone;		/* Whole zone should/has been scanned */
 	bool contended;			/* Signal lock or sched contention */
 	bool rescan;			/* Rescanning the same pageblock */
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 321a23c71a0b..f1c321e1d6d3 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -1096,24 +1096,6 @@ static int __fragmentation_index(unsigned int order, struct contig_page_info *in
 	return 1000 - div_u64( (1000+(div_u64(info->free_pages * 1000ULL, requested))), info->free_blocks_total);
 }
 
-/*
- * Calculates external fragmentation within a zone wrt the given order.
- * It is defined as the percentage of pages found in blocks of size
- * less than 1 << order. It returns values in range [0, 100].
- */
-int extfrag_for_order(struct zone *zone, unsigned int order)
-{
-	struct contig_page_info info;
-
-	fill_contig_page_info(zone, order, &info);
-	if (info.free_pages == 0)
-		return 0;
-
-	return div_u64((info.free_pages -
-			(info.free_blocks_suitable << order)) * 100,
-			info.free_pages);
-}
-
 /* Same as __fragmentation index but allocs contig_page_info on stack */
 int fragmentation_index(struct zone *zone, unsigned int order)
 {
-- 
2.27.0

