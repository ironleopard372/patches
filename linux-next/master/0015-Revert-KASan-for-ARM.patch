From c24fa6be2ce8d02e89554a04ab56d4e9c2b76b9d Mon Sep 17 00:00:00 2001
From: Nathan Chancellor <natechancellor@gmail.com>
Date: Thu, 29 Oct 2020 17:35:24 -0700
Subject: [PATCH 15/15] Revert KASan for ARM

This reverts:

fc2933c13374 ("ARM: 9020/1: mm: use correct section size macro to describe the FDT virtual address")
421015713b30 ("ARM: 9017/2: Enable KASan for ARM")
5615f69bc209 ("ARM: 9016/2: Initialize the mapping of KASan shadow memory")
c12366ba441d ("ARM: 9015/2: Define the virtual space of KASan's shadow region")
d6d51a96c7d6 ("ARM: 9014/2: Replace string mem* functions for KASan")
d5d44e7e3507 ("ARM: 9013/2: Disable KASan instrumentation for some code")
7a1be318f579 ("ARM: 9012/1: move device tree mapping out of linear region")
e9a2f8b599d0 ("ARM: 9011/1: centralize phys-to-virt conversion of DT/ATAGS address")

Link: https://lore.kernel.org/r/20201030002900.GA2248731@ubuntu-m3-large-x86/
Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
---
 Documentation/arm/memory.rst                  |  12 +-
 Documentation/dev-tools/kasan.rst             |   4 +-
 .../features/debug/KASAN/arch-support.txt     |   2 +-
 arch/arm/Kconfig                              |  10 -
 arch/arm/boot/compressed/Makefile             |   1 -
 arch/arm/boot/compressed/string.c             |  19 --
 arch/arm/include/asm/fixmap.h                 |   2 +-
 arch/arm/include/asm/kasan.h                  |  33 --
 arch/arm/include/asm/kasan_def.h              |  81 -----
 arch/arm/include/asm/memory.h                 |  10 -
 arch/arm/include/asm/pgalloc.h                |   8 +-
 arch/arm/include/asm/prom.h                   |   4 +-
 arch/arm/include/asm/string.h                 |  26 --
 arch/arm/include/asm/thread_info.h            |   8 -
 arch/arm/include/asm/uaccess-asm.h            |   2 +-
 arch/arm/kernel/atags.h                       |   4 +-
 arch/arm/kernel/atags_parse.c                 |   6 +-
 arch/arm/kernel/devtree.c                     |   6 +-
 arch/arm/kernel/entry-armv.S                  |   3 +-
 arch/arm/kernel/entry-common.S                |   9 +-
 arch/arm/kernel/head-common.S                 |   7 +-
 arch/arm/kernel/head.S                        |   5 +-
 arch/arm/kernel/setup.c                       |  21 +-
 arch/arm/kernel/unwind.c                      |   6 +-
 arch/arm/lib/memcpy.S                         |   3 -
 arch/arm/lib/memmove.S                        |   5 +-
 arch/arm/lib/memset.S                         |   3 -
 arch/arm/mm/Makefile                          |   5 -
 arch/arm/mm/init.c                            |   1 +
 arch/arm/mm/kasan_init.c                      | 291 ------------------
 arch/arm/mm/mmu.c                             |  38 +--
 arch/arm/mm/pgd.c                             |  16 +-
 arch/arm/mm/pv-fixup-asm.S                    |   4 +-
 arch/arm/vdso/Makefile                        |   2 -
 34 files changed, 43 insertions(+), 614 deletions(-)
 delete mode 100644 arch/arm/include/asm/kasan.h
 delete mode 100644 arch/arm/include/asm/kasan_def.h
 delete mode 100644 arch/arm/mm/kasan_init.c

diff --git a/Documentation/arm/memory.rst b/Documentation/arm/memory.rst
index 0cb1e2938823..0521b4ce5c96 100644
--- a/Documentation/arm/memory.rst
+++ b/Documentation/arm/memory.rst
@@ -45,14 +45,9 @@ fffe8000	fffeffff	DTCM mapping area for platforms with
 fffe0000	fffe7fff	ITCM mapping area for platforms with
 				ITCM mounted inside the CPU.
 
-ffc80000	ffefffff	Fixmap mapping region.  Addresses provided
+ffc00000	ffefffff	Fixmap mapping region.  Addresses provided
 				by fix_to_virt() will be located here.
 
-ffc00000	ffc7ffff	Guard region
-
-ff800000	ffbfffff	Permanent, fixed read-only mapping of the
-				firmware provided DT blob
-
 fee00000	feffffff	Mapping of PCI I/O space. This is a static
 				mapping within the vmalloc space.
 
@@ -77,11 +72,6 @@ MODULES_VADDR	MODULES_END-1	Kernel module space
 				Kernel modules inserted via insmod are
 				placed here using dynamic mappings.
 
-TASK_SIZE	MODULES_VADDR-1	KASAn shadow memory when KASan is in use.
-				The range from MODULES_VADDR to the top
-				of the memory is shadowed here with 1 bit
-				per byte of memory.
-
 00001000	TASK_SIZE-1	User space mappings
 				Per-thread mappings are placed here via
 				the mmap() system call.
diff --git a/Documentation/dev-tools/kasan.rst b/Documentation/dev-tools/kasan.rst
index b3e489064a18..2b68addaadcd 100644
--- a/Documentation/dev-tools/kasan.rst
+++ b/Documentation/dev-tools/kasan.rst
@@ -18,8 +18,8 @@ out-of-bounds accesses for global variables is only supported since Clang 11.
 
 Tag-based KASAN is only supported in Clang.
 
-Currently generic KASAN is supported for the x86_64, arm, arm64, xtensa, s390
-and riscv architectures, and tag-based KASAN is supported only for arm64.
+Currently generic KASAN is supported for the x86_64, arm64, xtensa, s390 and
+riscv architectures, and tag-based KASAN is supported only for arm64.
 
 Usage
 -----
diff --git a/Documentation/features/debug/KASAN/arch-support.txt b/Documentation/features/debug/KASAN/arch-support.txt
index b2288dc14b72..c3fe9b266e7b 100644
--- a/Documentation/features/debug/KASAN/arch-support.txt
+++ b/Documentation/features/debug/KASAN/arch-support.txt
@@ -8,7 +8,7 @@
     -----------------------
     |       alpha: | TODO |
     |         arc: | TODO |
-    |         arm: |  ok  |
+    |         arm: | TODO |
     |       arm64: |  ok  |
     |         c6x: | TODO |
     |        csky: | TODO |
diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 2c5eb5c2b310..fe2f17eb2b50 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -67,7 +67,6 @@ config ARM
 	select HAVE_ARCH_BITREVERSE if (CPU_32v7M || CPU_32v7) && !CPU_32v6
 	select HAVE_ARCH_JUMP_LABEL if !XIP_KERNEL && !CPU_ENDIAN_BE32 && MMU
 	select HAVE_ARCH_KGDB if !CPU_ENDIAN_BE32 && MMU
-	select HAVE_ARCH_KASAN if MMU && !XIP_KERNEL
 	select HAVE_ARCH_MMAP_RND_BITS if MMU
 	select HAVE_ARCH_SECCOMP
 	select HAVE_ARCH_SECCOMP_FILTER if AEABI && !OABI_COMPAT
@@ -1324,15 +1323,6 @@ config PAGE_OFFSET
 	default 0xB0000000 if VMSPLIT_3G_OPT
 	default 0xC0000000
 
-config KASAN_SHADOW_OFFSET
-	hex
-	depends on KASAN
-	default 0x1f000000 if PAGE_OFFSET=0x40000000
-	default 0x5f000000 if PAGE_OFFSET=0x80000000
-	default 0x9f000000 if PAGE_OFFSET=0xC0000000
-	default 0x8f000000 if PAGE_OFFSET=0xB0000000
-	default 0xffffffff
-
 config NR_CPUS
 	int "Maximum number of CPUs (2-32)"
 	range 2 32
diff --git a/arch/arm/boot/compressed/Makefile b/arch/arm/boot/compressed/Makefile
index a815b1ae990d..47f001ca5499 100644
--- a/arch/arm/boot/compressed/Makefile
+++ b/arch/arm/boot/compressed/Makefile
@@ -24,7 +24,6 @@ OBJS		+= hyp-stub.o
 endif
 
 GCOV_PROFILE		:= n
-KASAN_SANITIZE		:= n
 
 # Prevents link failures: __sanitizer_cov_trace_pc() is not linked in.
 KCOV_INSTRUMENT		:= n
diff --git a/arch/arm/boot/compressed/string.c b/arch/arm/boot/compressed/string.c
index 8c0fa276d994..ade5079bebbf 100644
--- a/arch/arm/boot/compressed/string.c
+++ b/arch/arm/boot/compressed/string.c
@@ -7,25 +7,6 @@
 
 #include <linux/string.h>
 
-/*
- * The decompressor is built without KASan but uses the same redirects as the
- * rest of the kernel when CONFIG_KASAN is enabled, defining e.g. memcpy()
- * to __memcpy() but since we are not linking with the main kernel string
- * library in the decompressor, that will lead to link failures.
- *
- * Undefine KASan's versions, define the wrapped functions and alias them to
- * the right names so that when e.g. __memcpy() appear in the code, it will
- * still be linked to this local version of memcpy().
- */
-#ifdef CONFIG_KASAN
-#undef memcpy
-#undef memmove
-#undef memset
-void *__memcpy(void *__dest, __const void *__src, size_t __n) __alias(memcpy);
-void *__memmove(void *__dest, __const void *__src, size_t count) __alias(memmove);
-void *__memset(void *s, int c, size_t count) __alias(memset);
-#endif
-
 void *memcpy(void *__dest, __const void *__src, size_t __n)
 {
 	int i = 0;
diff --git a/arch/arm/include/asm/fixmap.h b/arch/arm/include/asm/fixmap.h
index 9575b404019c..fc56fc3e1931 100644
--- a/arch/arm/include/asm/fixmap.h
+++ b/arch/arm/include/asm/fixmap.h
@@ -2,7 +2,7 @@
 #ifndef _ASM_FIXMAP_H
 #define _ASM_FIXMAP_H
 
-#define FIXADDR_START		0xffc80000UL
+#define FIXADDR_START		0xffc00000UL
 #define FIXADDR_END		0xfff00000UL
 #define FIXADDR_TOP		(FIXADDR_END - PAGE_SIZE)
 
diff --git a/arch/arm/include/asm/kasan.h b/arch/arm/include/asm/kasan.h
deleted file mode 100644
index 303c35df3135..000000000000
--- a/arch/arm/include/asm/kasan.h
+++ /dev/null
@@ -1,33 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * arch/arm/include/asm/kasan.h
- *
- * Copyright (c) 2015 Samsung Electronics Co., Ltd.
- * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
- *
- */
-
-#ifndef __ASM_KASAN_H
-#define __ASM_KASAN_H
-
-#ifdef CONFIG_KASAN
-
-#include <asm/kasan_def.h>
-
-#define KASAN_SHADOW_SCALE_SHIFT 3
-
-/*
- * The compiler uses a shadow offset assuming that addresses start
- * from 0. Kernel addresses don't start from 0, so shadow
- * for kernel really starts from 'compiler's shadow offset' +
- * ('kernel address space start' >> KASAN_SHADOW_SCALE_SHIFT)
- */
-
-asmlinkage void kasan_early_init(void);
-extern void kasan_init(void);
-
-#else
-static inline void kasan_init(void) { }
-#endif
-
-#endif
diff --git a/arch/arm/include/asm/kasan_def.h b/arch/arm/include/asm/kasan_def.h
deleted file mode 100644
index 5739605aa7cf..000000000000
--- a/arch/arm/include/asm/kasan_def.h
+++ /dev/null
@@ -1,81 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- *  arch/arm/include/asm/kasan_def.h
- *
- *  Copyright (c) 2018 Huawei Technologies Co., Ltd.
- *
- *  Author: Abbott Liu <liuwenliang@huawei.com>
- */
-
-#ifndef __ASM_KASAN_DEF_H
-#define __ASM_KASAN_DEF_H
-
-#ifdef CONFIG_KASAN
-
-/*
- * Define KASAN_SHADOW_OFFSET,KASAN_SHADOW_START and KASAN_SHADOW_END for
- * the Arm kernel address sanitizer. We are "stealing" lowmem (the 4GB
- * addressable by a 32bit architecture) out of the virtual address
- * space to use as shadow memory for KASan as follows:
- *
- * +----+ 0xffffffff
- * |    |							\
- * |    | |-> Static kernel image (vmlinux) BSS and page table
- * |    |/
- * +----+ PAGE_OFFSET
- * |    |							\
- * |    | |->  Loadable kernel modules virtual address space area
- * |    |/
- * +----+ MODULES_VADDR = KASAN_SHADOW_END
- * |    |						\
- * |    | |-> The shadow area of kernel virtual address.
- * |    |/
- * +----+->  TASK_SIZE (start of kernel space) = KASAN_SHADOW_START the
- * |    |\   shadow address of MODULES_VADDR
- * |    | |
- * |    | |
- * |    | |-> The user space area in lowmem. The kernel address
- * |    | |   sanitizer do not use this space, nor does it map it.
- * |    | |
- * |    | |
- * |    | |
- * |    | |
- * |    |/
- * ------ 0
- *
- * 1) KASAN_SHADOW_START
- *   This value begins with the MODULE_VADDR's shadow address. It is the
- *   start of kernel virtual space. Since we have modules to load, we need
- *   to cover also that area with shadow memory so we can find memory
- *   bugs in modules.
- *
- * 2) KASAN_SHADOW_END
- *   This value is the 0x100000000's shadow address: the mapping that would
- *   be after the end of the kernel memory at 0xffffffff. It is the end of
- *   kernel address sanitizer shadow area. It is also the start of the
- *   module area.
- *
- * 3) KASAN_SHADOW_OFFSET:
- *   This value is used to map an address to the corresponding shadow
- *   address by the following formula:
- *
- *	shadow_addr = (address >> 3) + KASAN_SHADOW_OFFSET;
- *
- *  As you would expect, >> 3 is equal to dividing by 8, meaning each
- *  byte in the shadow memory covers 8 bytes of kernel memory, so one
- *  bit shadow memory per byte of kernel memory is used.
- *
- *  The KASAN_SHADOW_OFFSET is provided in a Kconfig option depending
- *  on the VMSPLIT layout of the system: the kernel and userspace can
- *  split up lowmem in different ways according to needs, so we calculate
- *  the shadow offset depending on this.
- */
-
-#define KASAN_SHADOW_SCALE_SHIFT	3
-#define KASAN_SHADOW_OFFSET	_AC(CONFIG_KASAN_SHADOW_OFFSET, UL)
-#define KASAN_SHADOW_END	((UL(1) << (32 - KASAN_SHADOW_SCALE_SHIFT)) \
-				 + KASAN_SHADOW_OFFSET)
-#define KASAN_SHADOW_START      ((KASAN_SHADOW_END >> 3) + KASAN_SHADOW_OFFSET)
-
-#endif
-#endif
diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 38a163f50130..99035b5891ef 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -18,7 +18,6 @@
 #ifdef CONFIG_NEED_MACH_MEMORY_H
 #include <mach/memory.h>
 #endif
-#include <asm/kasan_def.h>
 
 /* PAGE_OFFSET - the virtual address of the start of the kernel image */
 #define PAGE_OFFSET		UL(CONFIG_PAGE_OFFSET)
@@ -29,11 +28,7 @@
  * TASK_SIZE - the maximum size of a user space task.
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area
  */
-#ifndef CONFIG_KASAN
 #define TASK_SIZE		(UL(CONFIG_PAGE_OFFSET) - UL(SZ_16M))
-#else
-#define TASK_SIZE		(KASAN_SHADOW_START)
-#endif
 #define TASK_UNMAPPED_BASE	ALIGN(TASK_SIZE / 3, SZ_16M)
 
 /*
@@ -72,10 +67,6 @@
  */
 #define XIP_VIRT_ADDR(physaddr)  (MODULES_VADDR + ((physaddr) & 0x000fffff))
 
-#define FDT_FIXED_BASE		UL(0xff800000)
-#define FDT_FIXED_SIZE		(2 * SECTION_SIZE)
-#define FDT_VIRT_BASE(physbase)	((void *)(FDT_FIXED_BASE | (physbase) % SECTION_SIZE))
-
 #if !defined(CONFIG_SMP) && !defined(CONFIG_ARM_LPAE)
 /*
  * Allow 16MB-aligned ioremap pages
@@ -116,7 +107,6 @@ extern unsigned long vectors_base;
 #define MODULES_VADDR		PAGE_OFFSET
 
 #define XIP_VIRT_ADDR(physaddr)  (physaddr)
-#define FDT_VIRT_BASE(physbase)  ((void *)(physbase))
 
 #endif /* !CONFIG_MMU */
 
diff --git a/arch/arm/include/asm/pgalloc.h b/arch/arm/include/asm/pgalloc.h
index fdee1f04f4f3..15f4674715f8 100644
--- a/arch/arm/include/asm/pgalloc.h
+++ b/arch/arm/include/asm/pgalloc.h
@@ -21,7 +21,6 @@
 #define _PAGE_KERNEL_TABLE	(PMD_TYPE_TABLE | PMD_BIT4 | PMD_DOMAIN(DOMAIN_KERNEL))
 
 #ifdef CONFIG_ARM_LPAE
-#define PGD_SIZE		(PTRS_PER_PGD * sizeof(pgd_t))
 
 static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)
 {
@@ -29,19 +28,14 @@ static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)
 }
 
 #else	/* !CONFIG_ARM_LPAE */
-#define PGD_SIZE		(PAGE_SIZE << 2)
 
 /*
  * Since we have only two-level page tables, these are trivial
  */
 #define pmd_alloc_one(mm,addr)		({ BUG(); ((pmd_t *)2); })
 #define pmd_free(mm, pmd)		do { } while (0)
-#ifdef CONFIG_KASAN
-/* The KASan core unconditionally calls pud_populate() on all architectures */
-#define pud_populate(mm,pmd,pte)	do { } while (0)
-#else
 #define pud_populate(mm,pmd,pte)	BUG()
-#endif
+
 #endif	/* CONFIG_ARM_LPAE */
 
 extern pgd_t *pgd_alloc(struct mm_struct *mm);
diff --git a/arch/arm/include/asm/prom.h b/arch/arm/include/asm/prom.h
index 402e3f34c7ed..1e36c40533c1 100644
--- a/arch/arm/include/asm/prom.h
+++ b/arch/arm/include/asm/prom.h
@@ -9,12 +9,12 @@
 
 #ifdef CONFIG_OF
 
-extern const struct machine_desc *setup_machine_fdt(void *dt_virt);
+extern const struct machine_desc *setup_machine_fdt(unsigned int dt_phys);
 extern void __init arm_dt_init_cpu_maps(void);
 
 #else /* CONFIG_OF */
 
-static inline const struct machine_desc *setup_machine_fdt(void *dt_virt)
+static inline const struct machine_desc *setup_machine_fdt(unsigned int dt_phys)
 {
 	return NULL;
 }
diff --git a/arch/arm/include/asm/string.h b/arch/arm/include/asm/string.h
index 6c607c68f3ad..111a1d8a41dd 100644
--- a/arch/arm/include/asm/string.h
+++ b/arch/arm/include/asm/string.h
@@ -5,9 +5,6 @@
 /*
  * We don't do inline string functions, since the
  * optimised inline asm versions are not small.
- *
- * The __underscore versions of some functions are for KASan to be able
- * to replace them with instrumented versions.
  */
 
 #define __HAVE_ARCH_STRRCHR
@@ -18,18 +15,15 @@ extern char * strchr(const char * s, int c);
 
 #define __HAVE_ARCH_MEMCPY
 extern void * memcpy(void *, const void *, __kernel_size_t);
-extern void *__memcpy(void *dest, const void *src, __kernel_size_t n);
 
 #define __HAVE_ARCH_MEMMOVE
 extern void * memmove(void *, const void *, __kernel_size_t);
-extern void *__memmove(void *dest, const void *src, __kernel_size_t n);
 
 #define __HAVE_ARCH_MEMCHR
 extern void * memchr(const void *, int, __kernel_size_t);
 
 #define __HAVE_ARCH_MEMSET
 extern void * memset(void *, int, __kernel_size_t);
-extern void *__memset(void *s, int c, __kernel_size_t n);
 
 #define __HAVE_ARCH_MEMSET32
 extern void *__memset32(uint32_t *, uint32_t v, __kernel_size_t);
@@ -45,24 +39,4 @@ static inline void *memset64(uint64_t *p, uint64_t v, __kernel_size_t n)
 	return __memset64(p, v, n * 8, v >> 32);
 }
 
-/*
- * For files that are not instrumented (e.g. mm/slub.c) we
- * must use non-instrumented versions of the mem*
- * functions named __memcpy() etc. All such kernel code has
- * been tagged with KASAN_SANITIZE_file.o = n, which means
- * that the address sanitization argument isn't passed to the
- * compiler, and __SANITIZE_ADDRESS__ is not set. As a result
- * these defines kick in.
- */
-#if defined(CONFIG_KASAN) && !defined(__SANITIZE_ADDRESS__)
-#define memcpy(dst, src, len) __memcpy(dst, src, len)
-#define memmove(dst, src, len) __memmove(dst, src, len)
-#define memset(s, c, n) __memset(s, c, n)
-
-#ifndef __NO_FORTIFY
-#define __NO_FORTIFY /* FORTIFY_SOURCE uses __builtin_memcpy, etc. */
-#endif
-
-#endif
-
 #endif
diff --git a/arch/arm/include/asm/thread_info.h b/arch/arm/include/asm/thread_info.h
index 56fae7861fd3..536b6b979f63 100644
--- a/arch/arm/include/asm/thread_info.h
+++ b/arch/arm/include/asm/thread_info.h
@@ -13,15 +13,7 @@
 #include <asm/fpstate.h>
 #include <asm/page.h>
 
-#ifdef CONFIG_KASAN
-/*
- * KASan uses a lot of extra stack space so the thread size order needs to
- * be increased.
- */
-#define THREAD_SIZE_ORDER	2
-#else
 #define THREAD_SIZE_ORDER	1
-#endif
 #define THREAD_SIZE		(PAGE_SIZE << THREAD_SIZE_ORDER)
 #define THREAD_START_SP		(THREAD_SIZE - 8)
 
diff --git a/arch/arm/include/asm/uaccess-asm.h b/arch/arm/include/asm/uaccess-asm.h
index e6eb7a2aaf1e..907571fd05c6 100644
--- a/arch/arm/include/asm/uaccess-asm.h
+++ b/arch/arm/include/asm/uaccess-asm.h
@@ -85,7 +85,7 @@
 	 */
 	.macro	uaccess_entry, tsk, tmp0, tmp1, tmp2, disable
 	ldr	\tmp1, [\tsk, #TI_ADDR_LIMIT]
-	ldr	\tmp2, =TASK_SIZE
+	mov	\tmp2, #TASK_SIZE
 	str	\tmp2, [\tsk, #TI_ADDR_LIMIT]
  DACR(	mrc	p15, 0, \tmp0, c3, c0, 0)
  DACR(	str	\tmp0, [sp, #SVC_DACR])
diff --git a/arch/arm/kernel/atags.h b/arch/arm/kernel/atags.h
index f2819c25b602..067e12edc341 100644
--- a/arch/arm/kernel/atags.h
+++ b/arch/arm/kernel/atags.h
@@ -2,11 +2,11 @@
 void convert_to_tag_list(struct tag *tags);
 
 #ifdef CONFIG_ATAGS
-const struct machine_desc *setup_machine_tags(void *__atags_vaddr,
+const struct machine_desc *setup_machine_tags(phys_addr_t __atags_pointer,
 	unsigned int machine_nr);
 #else
 static inline const struct machine_desc * __init __noreturn
-setup_machine_tags(void *__atags_vaddr, unsigned int machine_nr)
+setup_machine_tags(phys_addr_t __atags_pointer, unsigned int machine_nr)
 {
 	early_print("no ATAGS support: can't continue\n");
 	while (true);
diff --git a/arch/arm/kernel/atags_parse.c b/arch/arm/kernel/atags_parse.c
index 373b61f9a4f0..6c12d9fe694e 100644
--- a/arch/arm/kernel/atags_parse.c
+++ b/arch/arm/kernel/atags_parse.c
@@ -174,7 +174,7 @@ static void __init squash_mem_tags(struct tag *tag)
 }
 
 const struct machine_desc * __init
-setup_machine_tags(void *atags_vaddr, unsigned int machine_nr)
+setup_machine_tags(phys_addr_t __atags_pointer, unsigned int machine_nr)
 {
 	struct tag *tags = (struct tag *)&default_tags;
 	const struct machine_desc *mdesc = NULL, *p;
@@ -195,8 +195,8 @@ setup_machine_tags(void *atags_vaddr, unsigned int machine_nr)
 	if (!mdesc)
 		return NULL;
 
-	if (atags_vaddr)
-		tags = atags_vaddr;
+	if (__atags_pointer)
+		tags = phys_to_virt(__atags_pointer);
 	else if (mdesc->atag_offset)
 		tags = (void *)(PAGE_OFFSET + mdesc->atag_offset);
 
diff --git a/arch/arm/kernel/devtree.c b/arch/arm/kernel/devtree.c
index 28311dd0fee6..7f0745a97e20 100644
--- a/arch/arm/kernel/devtree.c
+++ b/arch/arm/kernel/devtree.c
@@ -203,12 +203,12 @@ static const void * __init arch_get_next_mach(const char *const **match)
 
 /**
  * setup_machine_fdt - Machine setup when an dtb was passed to the kernel
- * @dt_virt: virtual address of dt blob
+ * @dt_phys: physical address of dt blob
  *
  * If a dtb was passed to the kernel in r2, then use it to choose the
  * correct machine_desc and to setup the system.
  */
-const struct machine_desc * __init setup_machine_fdt(void *dt_virt)
+const struct machine_desc * __init setup_machine_fdt(unsigned int dt_phys)
 {
 	const struct machine_desc *mdesc, *mdesc_best = NULL;
 
@@ -221,7 +221,7 @@ const struct machine_desc * __init setup_machine_fdt(void *dt_virt)
 	mdesc_best = &__mach_desc_GENERIC_DT;
 #endif
 
-	if (!dt_virt || !early_init_dt_verify(dt_virt))
+	if (!dt_phys || !early_init_dt_verify(phys_to_virt(dt_phys)))
 		return NULL;
 
 	mdesc = of_flat_dt_match_machine(mdesc_best, arch_get_next_mach);
diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
index c4220f51fcf3..55a47df04773 100644
--- a/arch/arm/kernel/entry-armv.S
+++ b/arch/arm/kernel/entry-armv.S
@@ -427,8 +427,7 @@ ENDPROC(__fiq_abt)
 	@ if it was interrupted in a critical region.  Here we
 	@ perform a quick test inline since it should be false
 	@ 99.9999% of the time.  The rest is done out of line.
-	ldr	r0, =TASK_SIZE
-	cmp	r4, r0
+	cmp	r4, #TASK_SIZE
 	blhs	kuser_cmpxchg64_fixup
 #endif
 #endif
diff --git a/arch/arm/kernel/entry-common.S b/arch/arm/kernel/entry-common.S
index fee279e28a72..271cb8a1eba1 100644
--- a/arch/arm/kernel/entry-common.S
+++ b/arch/arm/kernel/entry-common.S
@@ -50,8 +50,7 @@ __ret_fast_syscall:
  UNWIND(.cantunwind	)
 	disable_irq_notrace			@ disable interrupts
 	ldr	r2, [tsk, #TI_ADDR_LIMIT]
-	ldr	r1, =TASK_SIZE
-	cmp	r2, r1
+	cmp	r2, #TASK_SIZE
 	blne	addr_limit_check_failed
 	ldr	r1, [tsk, #TI_FLAGS]		@ re-check for syscall tracing
 	tst	r1, #_TIF_SYSCALL_WORK | _TIF_WORK_MASK
@@ -88,8 +87,7 @@ __ret_fast_syscall:
 #endif
 	disable_irq_notrace			@ disable interrupts
 	ldr	r2, [tsk, #TI_ADDR_LIMIT]
-	ldr     r1, =TASK_SIZE
-	cmp     r2, r1
+	cmp	r2, #TASK_SIZE
 	blne	addr_limit_check_failed
 	ldr	r1, [tsk, #TI_FLAGS]		@ re-check for syscall tracing
 	tst	r1, #_TIF_SYSCALL_WORK | _TIF_WORK_MASK
@@ -130,8 +128,7 @@ ret_slow_syscall:
 	disable_irq_notrace			@ disable interrupts
 ENTRY(ret_to_user_from_irq)
 	ldr	r2, [tsk, #TI_ADDR_LIMIT]
-	ldr     r1, =TASK_SIZE
-	cmp	r2, r1
+	cmp	r2, #TASK_SIZE
 	blne	addr_limit_check_failed
 	ldr	r1, [tsk, #TI_FLAGS]
 	tst	r1, #_TIF_WORK_MASK
diff --git a/arch/arm/kernel/head-common.S b/arch/arm/kernel/head-common.S
index 89c80154b9ef..4a3982812a40 100644
--- a/arch/arm/kernel/head-common.S
+++ b/arch/arm/kernel/head-common.S
@@ -95,7 +95,7 @@ __mmap_switched:
  THUMB(	ldmia	r4!, {r0, r1, r2, r3} )
  THUMB(	mov	sp, r3 )
 	sub	r2, r2, r1
-	bl	__memcpy			@ copy .data to RAM
+	bl	memcpy				@ copy .data to RAM
 #endif
 
    ARM(	ldmia	r4!, {r0, r1, sp} )
@@ -103,7 +103,7 @@ __mmap_switched:
  THUMB(	mov	sp, r3 )
 	sub	r2, r1, r0
 	mov	r1, #0
-	bl	__memset			@ clear .bss
+	bl	memset				@ clear .bss
 
 	ldmia	r4, {r0, r1, r2, r3}
 	str	r9, [r0]			@ Save processor ID
@@ -111,9 +111,6 @@ __mmap_switched:
 	str	r8, [r2]			@ Save atags pointer
 	cmp	r3, #0
 	strne	r10, [r3]			@ Save control register values
-#ifdef CONFIG_KASAN
-	bl	kasan_early_init
-#endif
 	mov	lr, #0
 	b	start_kernel
 ENDPROC(__mmap_switched)
diff --git a/arch/arm/kernel/head.S b/arch/arm/kernel/head.S
index 9b18d8c66129..f8904227e7fd 100644
--- a/arch/arm/kernel/head.S
+++ b/arch/arm/kernel/head.S
@@ -275,8 +275,9 @@ __create_page_tables:
 	 */
 	mov	r0, r2, lsr #SECTION_SHIFT
 	movs	r0, r0, lsl #SECTION_SHIFT
-	ldrne	r3, =FDT_FIXED_BASE >> (SECTION_SHIFT - PMD_ORDER)
-	addne	r3, r3, r4
+	subne	r3, r0, r8
+	addne	r3, r3, #PAGE_OFFSET
+	addne	r3, r4, r3, lsr #(SECTION_SHIFT - PMD_ORDER)
 	orrne	r6, r7, r0
 	strne	r6, [r3], #1 << PMD_ORDER
 	addne	r6, r6, #1 << SECTION_SHIFT
diff --git a/arch/arm/kernel/setup.c b/arch/arm/kernel/setup.c
index d32f7652a5bf..3f65d0ac9f63 100644
--- a/arch/arm/kernel/setup.c
+++ b/arch/arm/kernel/setup.c
@@ -18,7 +18,6 @@
 #include <linux/of_platform.h>
 #include <linux/init.h>
 #include <linux/kexec.h>
-#include <linux/libfdt.h>
 #include <linux/of_fdt.h>
 #include <linux/cpu.h>
 #include <linux/interrupt.h>
@@ -59,7 +58,6 @@
 #include <asm/unwind.h>
 #include <asm/memblock.h>
 #include <asm/virt.h>
-#include <asm/kasan.h>
 
 #include "atags.h"
 
@@ -1083,27 +1081,19 @@ void __init hyp_mode_check(void)
 
 void __init setup_arch(char **cmdline_p)
 {
-	const struct machine_desc *mdesc = NULL;
-	void *atags_vaddr = NULL;
-
-	if (__atags_pointer)
-		atags_vaddr = FDT_VIRT_BASE(__atags_pointer);
+	const struct machine_desc *mdesc;
 
 	setup_processor();
-	if (atags_vaddr) {
-		mdesc = setup_machine_fdt(atags_vaddr);
-		if (mdesc)
-			memblock_reserve(__atags_pointer,
-					 fdt_totalsize(atags_vaddr));
-	}
+	mdesc = setup_machine_fdt(__atags_pointer);
 	if (!mdesc)
-		mdesc = setup_machine_tags(atags_vaddr, __machine_arch_type);
+		mdesc = setup_machine_tags(__atags_pointer, __machine_arch_type);
 	if (!mdesc) {
 		early_print("\nError: invalid dtb and unrecognized/unsupported machine ID\n");
 		early_print("  r1=0x%08x, r2=0x%08x\n", __machine_arch_type,
 			    __atags_pointer);
 		if (__atags_pointer)
-			early_print("  r2[]=%*ph\n", 16, atags_vaddr);
+			early_print("  r2[]=%*ph\n", 16,
+				    phys_to_virt(__atags_pointer));
 		dump_machine_table();
 	}
 
@@ -1146,7 +1136,6 @@ void __init setup_arch(char **cmdline_p)
 	early_ioremap_reset();
 
 	paging_init(mdesc);
-	kasan_init();
 	request_standard_resources(mdesc);
 
 	if (mdesc->restart)
diff --git a/arch/arm/kernel/unwind.c b/arch/arm/kernel/unwind.c
index f35eb584a18a..d2bd0df2318d 100644
--- a/arch/arm/kernel/unwind.c
+++ b/arch/arm/kernel/unwind.c
@@ -236,11 +236,7 @@ static int unwind_pop_register(struct unwind_ctrl_block *ctrl,
 		if (*vsp >= (unsigned long *)ctrl->sp_high)
 			return -URC_FAILURE;
 
-	/* Use READ_ONCE_NOCHECK here to avoid this memory access
-	 * from being tracked by KASAN.
-	 */
-	ctrl->vrs[reg] = READ_ONCE_NOCHECK(*(*vsp));
-	(*vsp)++;
+	ctrl->vrs[reg] = *(*vsp)++;
 	return URC_OK;
 }
 
diff --git a/arch/arm/lib/memcpy.S b/arch/arm/lib/memcpy.S
index ad4625d16e11..09a333153dc6 100644
--- a/arch/arm/lib/memcpy.S
+++ b/arch/arm/lib/memcpy.S
@@ -58,8 +58,6 @@
 
 /* Prototype: void *memcpy(void *dest, const void *src, size_t n); */
 
-.weak memcpy
-ENTRY(__memcpy)
 ENTRY(mmiocpy)
 ENTRY(memcpy)
 
@@ -67,4 +65,3 @@ ENTRY(memcpy)
 
 ENDPROC(memcpy)
 ENDPROC(mmiocpy)
-ENDPROC(__memcpy)
diff --git a/arch/arm/lib/memmove.S b/arch/arm/lib/memmove.S
index fd123ea5a5a4..b50e5770fb44 100644
--- a/arch/arm/lib/memmove.S
+++ b/arch/arm/lib/memmove.S
@@ -24,14 +24,12 @@
  * occurring in the opposite direction.
  */
 
-.weak memmove
-ENTRY(__memmove)
 ENTRY(memmove)
 	UNWIND(	.fnstart			)
 
 		subs	ip, r0, r1
 		cmphi	r2, ip
-		bls	__memcpy
+		bls	memcpy
 
 		stmfd	sp!, {r0, r4, lr}
 	UNWIND(	.fnend				)
@@ -224,4 +222,3 @@ ENTRY(memmove)
 18:		backward_copy_shift	push=24	pull=8
 
 ENDPROC(memmove)
-ENDPROC(__memmove)
diff --git a/arch/arm/lib/memset.S b/arch/arm/lib/memset.S
index 0e7ff0423f50..6ca4535c47fb 100644
--- a/arch/arm/lib/memset.S
+++ b/arch/arm/lib/memset.S
@@ -13,8 +13,6 @@
 	.text
 	.align	5
 
-.weak memset
-ENTRY(__memset)
 ENTRY(mmioset)
 ENTRY(memset)
 UNWIND( .fnstart         )
@@ -134,7 +132,6 @@ UNWIND( .fnstart            )
 UNWIND( .fnend   )
 ENDPROC(memset)
 ENDPROC(mmioset)
-ENDPROC(__memset)
 
 ENTRY(__memset32)
 UNWIND( .fnstart         )
diff --git a/arch/arm/mm/Makefile b/arch/arm/mm/Makefile
index 4536159bc8fa..7cb1699fbfc4 100644
--- a/arch/arm/mm/Makefile
+++ b/arch/arm/mm/Makefile
@@ -7,7 +7,6 @@ obj-y				:= extable.o fault.o init.o iomap.o
 obj-y				+= dma-mapping$(MMUEXT).o
 obj-$(CONFIG_MMU)		+= fault-armv.o flush.o idmap.o ioremap.o \
 				   mmap.o pgd.o mmu.o pageattr.o
-KASAN_SANITIZE_mmu.o		:= n
 
 ifneq ($(CONFIG_MMU),y)
 obj-y				+= nommu.o
@@ -17,7 +16,6 @@ endif
 obj-$(CONFIG_ARM_PTDUMP_CORE)	+= dump.o
 obj-$(CONFIG_ARM_PTDUMP_DEBUGFS)	+= ptdump_debugfs.o
 obj-$(CONFIG_MODULES)		+= proc-syms.o
-KASAN_SANITIZE_physaddr.o	:= n
 obj-$(CONFIG_DEBUG_VIRTUAL)	+= physaddr.o
 
 obj-$(CONFIG_ALIGNMENT_TRAP)	+= alignment.o
@@ -113,6 +111,3 @@ obj-$(CONFIG_CACHE_L2X0_PMU)	+= cache-l2x0-pmu.o
 obj-$(CONFIG_CACHE_XSC3L2)	+= cache-xsc3l2.o
 obj-$(CONFIG_CACHE_TAUROS2)	+= cache-tauros2.o
 obj-$(CONFIG_CACHE_UNIPHIER)	+= cache-uniphier.o
-
-KASAN_SANITIZE_kasan_init.o	:= n
-obj-$(CONFIG_KASAN)		+= kasan_init.o
diff --git a/arch/arm/mm/init.c b/arch/arm/mm/init.c
index a391804c7ce3..d57112a276f5 100644
--- a/arch/arm/mm/init.c
+++ b/arch/arm/mm/init.c
@@ -223,6 +223,7 @@ void __init arm_memblock_init(const struct machine_desc *mdesc)
 	if (mdesc->reserve)
 		mdesc->reserve();
 
+	early_init_fdt_reserve_self();
 	early_init_fdt_scan_reserved_mem();
 
 	/* reserve memory for DMA contiguous allocations */
diff --git a/arch/arm/mm/kasan_init.c b/arch/arm/mm/kasan_init.c
deleted file mode 100644
index 9c348042a724..000000000000
--- a/arch/arm/mm/kasan_init.c
+++ /dev/null
@@ -1,291 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0-only
-/*
- * This file contains kasan initialization code for ARM.
- *
- * Copyright (c) 2018 Samsung Electronics Co., Ltd.
- * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
- * Author: Linus Walleij <linus.walleij@linaro.org>
- */
-
-#define pr_fmt(fmt) "kasan: " fmt
-#include <linux/kasan.h>
-#include <linux/kernel.h>
-#include <linux/memblock.h>
-#include <linux/sched/task.h>
-#include <linux/start_kernel.h>
-#include <linux/pgtable.h>
-#include <asm/cputype.h>
-#include <asm/highmem.h>
-#include <asm/mach/map.h>
-#include <asm/memory.h>
-#include <asm/page.h>
-#include <asm/pgalloc.h>
-#include <asm/procinfo.h>
-#include <asm/proc-fns.h>
-
-#include "mm.h"
-
-static pgd_t tmp_pgd_table[PTRS_PER_PGD] __initdata __aligned(PGD_SIZE);
-
-pmd_t tmp_pmd_table[PTRS_PER_PMD] __page_aligned_bss;
-
-static __init void *kasan_alloc_block(size_t size)
-{
-	return memblock_alloc_try_nid(size, size, __pa(MAX_DMA_ADDRESS),
-				      MEMBLOCK_ALLOC_KASAN, NUMA_NO_NODE);
-}
-
-static void __init kasan_pte_populate(pmd_t *pmdp, unsigned long addr,
-				      unsigned long end, bool early)
-{
-	unsigned long next;
-	pte_t *ptep = pte_offset_kernel(pmdp, addr);
-
-	do {
-		pte_t entry;
-		void *p;
-
-		next = addr + PAGE_SIZE;
-
-		if (!early) {
-			if (!pte_none(READ_ONCE(*ptep)))
-				continue;
-
-			p = kasan_alloc_block(PAGE_SIZE);
-			if (!p) {
-				panic("%s failed to allocate shadow page for address 0x%lx\n",
-				      __func__, addr);
-				return;
-			}
-			memset(p, KASAN_SHADOW_INIT, PAGE_SIZE);
-			entry = pfn_pte(virt_to_pfn(p),
-					__pgprot(pgprot_val(PAGE_KERNEL)));
-		} else if (pte_none(READ_ONCE(*ptep))) {
-			/*
-			 * The early shadow memory is mapping all KASan
-			 * operations to one and the same page in memory,
-			 * "kasan_early_shadow_page" so that the instrumentation
-			 * will work on a scratch area until we can set up the
-			 * proper KASan shadow memory.
-			 */
-			entry = pfn_pte(virt_to_pfn(kasan_early_shadow_page),
-					__pgprot(_L_PTE_DEFAULT | L_PTE_DIRTY | L_PTE_XN));
-		} else {
-			/*
-			 * Early shadow mappings are PMD_SIZE aligned, so if the
-			 * first entry is already set, they must all be set.
-			 */
-			return;
-		}
-
-		set_pte_at(&init_mm, addr, ptep, entry);
-	} while (ptep++, addr = next, addr != end);
-}
-
-/*
- * The pmd (page middle directory) is only used on LPAE
- */
-static void __init kasan_pmd_populate(pud_t *pudp, unsigned long addr,
-				      unsigned long end, bool early)
-{
-	unsigned long next;
-	pmd_t *pmdp = pmd_offset(pudp, addr);
-
-	do {
-		if (pmd_none(*pmdp)) {
-			/*
-			 * We attempt to allocate a shadow block for the PMDs
-			 * used by the PTEs for this address if it isn't already
-			 * allocated.
-			 */
-			void *p = early ? kasan_early_shadow_pte :
-				kasan_alloc_block(PAGE_SIZE);
-
-			if (!p) {
-				panic("%s failed to allocate shadow block for address 0x%lx\n",
-				      __func__, addr);
-				return;
-			}
-			pmd_populate_kernel(&init_mm, pmdp, p);
-			flush_pmd_entry(pmdp);
-		}
-
-		next = pmd_addr_end(addr, end);
-		kasan_pte_populate(pmdp, addr, next, early);
-	} while (pmdp++, addr = next, addr != end);
-}
-
-static void __init kasan_pgd_populate(unsigned long addr, unsigned long end,
-				      bool early)
-{
-	unsigned long next;
-	pgd_t *pgdp;
-	p4d_t *p4dp;
-	pud_t *pudp;
-
-	pgdp = pgd_offset_k(addr);
-
-	do {
-		/*
-		 * Allocate and populate the shadow block of p4d folded into
-		 * pud folded into pmd if it doesn't already exist
-		 */
-		if (!early && pgd_none(*pgdp)) {
-			void *p = kasan_alloc_block(PAGE_SIZE);
-
-			if (!p) {
-				panic("%s failed to allocate shadow block for address 0x%lx\n",
-				      __func__, addr);
-				return;
-			}
-			pgd_populate(&init_mm, pgdp, p);
-		}
-
-		next = pgd_addr_end(addr, end);
-		/*
-		 * We just immediately jump over the p4d and pud page
-		 * directories since we believe ARM32 will never gain four
-		 * nor five level page tables.
-		 */
-		p4dp = p4d_offset(pgdp, addr);
-		pudp = pud_offset(p4dp, addr);
-
-		kasan_pmd_populate(pudp, addr, next, early);
-	} while (pgdp++, addr = next, addr != end);
-}
-
-extern struct proc_info_list *lookup_processor_type(unsigned int);
-
-void __init kasan_early_init(void)
-{
-	struct proc_info_list *list;
-
-	/*
-	 * locate processor in the list of supported processor
-	 * types.  The linker builds this table for us from the
-	 * entries in arch/arm/mm/proc-*.S
-	 */
-	list = lookup_processor_type(read_cpuid_id());
-	if (list) {
-#ifdef MULTI_CPU
-		processor = *list->proc;
-#endif
-	}
-
-	BUILD_BUG_ON((KASAN_SHADOW_END - (1UL << 29)) != KASAN_SHADOW_OFFSET);
-	/*
-	 * We walk the page table and set all of the shadow memory to point
-	 * to the scratch page.
-	 */
-	kasan_pgd_populate(KASAN_SHADOW_START, KASAN_SHADOW_END, true);
-}
-
-static void __init clear_pgds(unsigned long start,
-			unsigned long end)
-{
-	for (; start && start < end; start += PMD_SIZE)
-		pmd_clear(pmd_off_k(start));
-}
-
-static int __init create_mapping(void *start, void *end)
-{
-	void *shadow_start, *shadow_end;
-
-	shadow_start = kasan_mem_to_shadow(start);
-	shadow_end = kasan_mem_to_shadow(end);
-
-	pr_info("Mapping kernel virtual memory block: %px-%px at shadow: %px-%px\n",
-		start, end, shadow_start, shadow_end);
-
-	kasan_pgd_populate((unsigned long)shadow_start & PAGE_MASK,
-			   PAGE_ALIGN((unsigned long)shadow_end), false);
-	return 0;
-}
-
-void __init kasan_init(void)
-{
-	phys_addr_t pa_start, pa_end;
-	u64 i;
-
-	/*
-	 * We are going to perform proper setup of shadow memory.
-	 *
-	 * At first we should unmap early shadow (clear_pgds() call bellow).
-	 * However, instrumented code can't execute without shadow memory.
-	 *
-	 * To keep the early shadow memory MMU tables around while setting up
-	 * the proper shadow memory, we copy swapper_pg_dir (the initial page
-	 * table) to tmp_pgd_table and use that to keep the early shadow memory
-	 * mapped until the full shadow setup is finished. Then we swap back
-	 * to the proper swapper_pg_dir.
-	 */
-
-	memcpy(tmp_pgd_table, swapper_pg_dir, sizeof(tmp_pgd_table));
-#ifdef CONFIG_ARM_LPAE
-	/* We need to be in the same PGD or this won't work */
-	BUILD_BUG_ON(pgd_index(KASAN_SHADOW_START) !=
-		     pgd_index(KASAN_SHADOW_END));
-	memcpy(tmp_pmd_table,
-	       pgd_page_vaddr(*pgd_offset_k(KASAN_SHADOW_START)),
-	       sizeof(tmp_pmd_table));
-	set_pgd(&tmp_pgd_table[pgd_index(KASAN_SHADOW_START)],
-		__pgd(__pa(tmp_pmd_table) | PMD_TYPE_TABLE | L_PGD_SWAPPER));
-#endif
-	cpu_switch_mm(tmp_pgd_table, &init_mm);
-	local_flush_tlb_all();
-
-	clear_pgds(KASAN_SHADOW_START, KASAN_SHADOW_END);
-
-	kasan_populate_early_shadow(kasan_mem_to_shadow((void *)VMALLOC_START),
-				    kasan_mem_to_shadow((void *)-1UL) + 1);
-
-	for_each_mem_range(i, &pa_start, &pa_end) {
-		void *start = __va(pa_start);
-		void *end = __va(pa_end);
-
-		/* Do not attempt to shadow highmem */
-		if (pa_start >= arm_lowmem_limit) {
-			pr_info("Skip highmem block at %pa-%pa\n", &pa_start, &pa_end);
-			continue;
-		}
-		if (pa_end > arm_lowmem_limit) {
-			pr_info("Truncating shadow for memory block at %pa-%pa to lowmem region at %pa\n",
-				&pa_start, &pa_end, &arm_lowmem_limit);
-			end = __va(arm_lowmem_limit);
-		}
-		if (start >= end) {
-			pr_info("Skipping invalid memory block %pa-%pa (virtual %p-%p)\n",
-				&pa_start, &pa_end, start, end);
-			continue;
-		}
-
-		create_mapping(start, end);
-	}
-
-	/*
-	 * 1. The module global variables are in MODULES_VADDR ~ MODULES_END,
-	 *    so we need to map this area.
-	 * 2. PKMAP_BASE ~ PKMAP_BASE+PMD_SIZE's shadow and MODULES_VADDR
-	 *    ~ MODULES_END's shadow is in the same PMD_SIZE, so we can't
-	 *    use kasan_populate_zero_shadow.
-	 */
-	create_mapping((void *)MODULES_VADDR, (void *)(PKMAP_BASE + PMD_SIZE));
-
-	/*
-	 * KAsan may reuse the contents of kasan_early_shadow_pte directly, so
-	 * we should make sure that it maps the zero page read-only.
-	 */
-	for (i = 0; i < PTRS_PER_PTE; i++)
-		set_pte_at(&init_mm, KASAN_SHADOW_START + i*PAGE_SIZE,
-			   &kasan_early_shadow_pte[i],
-			   pfn_pte(virt_to_pfn(kasan_early_shadow_page),
-				__pgprot(pgprot_val(PAGE_KERNEL)
-					 | L_PTE_RDONLY)));
-
-	cpu_switch_mm(swapper_pg_dir, &init_mm);
-	local_flush_tlb_all();
-
-	memset(kasan_early_shadow_page, 0, PAGE_SIZE);
-	pr_info("Kernel address sanitizer initialized\n");
-	init_task.kasan_depth = 0;
-}
diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index c06ebfbc48c4..ab69250a86bc 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -29,7 +29,6 @@
 #include <asm/procinfo.h>
 #include <asm/memory.h>
 #include <asm/pgalloc.h>
-#include <asm/kasan_def.h>
 
 #include <asm/mach/arch.h>
 #include <asm/mach/map.h>
@@ -40,8 +39,6 @@
 #include "mm.h"
 #include "tcm.h"
 
-extern unsigned long __atags_pointer;
-
 /*
  * empty_zero_page is a special page that is used for
  * zero-initialized data and COW.
@@ -949,7 +946,7 @@ static void __init create_mapping(struct map_desc *md)
 		return;
 	}
 
-	if (md->type == MT_DEVICE &&
+	if ((md->type == MT_DEVICE || md->type == MT_ROM) &&
 	    md->virtual >= PAGE_OFFSET && md->virtual < FIXADDR_START &&
 	    (md->virtual < VMALLOC_START || md->virtual >= VMALLOC_END)) {
 		pr_warn("BUG: mapping for 0x%08llx at 0x%08lx out of vmalloc space\n",
@@ -1256,25 +1253,8 @@ static inline void prepare_page_table(void)
 	/*
 	 * Clear out all the mappings below the kernel image.
 	 */
-#ifdef CONFIG_KASAN
-	/*
-	 * KASan's shadow memory inserts itself between the TASK_SIZE
-	 * and MODULES_VADDR. Do not clear the KASan shadow memory mappings.
-	 */
-	for (addr = 0; addr < KASAN_SHADOW_START; addr += PMD_SIZE)
-		pmd_clear(pmd_off_k(addr));
-	/*
-	 * Skip over the KASan shadow area. KASAN_SHADOW_END is sometimes
-	 * equal to MODULES_VADDR and then we exit the pmd clearing. If we
-	 * are using a thumb-compiled kernel, there there will be 8MB more
-	 * to clear as KASan always offset to 16 MB below MODULES_VADDR.
-	 */
-	for (addr = KASAN_SHADOW_END; addr < MODULES_VADDR; addr += PMD_SIZE)
-		pmd_clear(pmd_off_k(addr));
-#else
 	for (addr = 0; addr < MODULES_VADDR; addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
-#endif
 
 #ifdef CONFIG_XIP_KERNEL
 	/* The XIP kernel is mapped in the module area -- skip over it */
@@ -1353,15 +1333,6 @@ static void __init devicemaps_init(const struct machine_desc *mdesc)
 	for (addr = VMALLOC_START; addr < (FIXADDR_TOP & PMD_MASK); addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
 
-	if (__atags_pointer) {
-		/* create a read-only mapping of the device tree */
-		map.pfn = __phys_to_pfn(__atags_pointer & SECTION_MASK);
-		map.virtual = FDT_FIXED_BASE;
-		map.length = FDT_FIXED_SIZE;
-		map.type = MT_ROM;
-		create_mapping(&map);
-	}
-
 	/*
 	 * Map the kernel if it is XIP.
 	 * It is always first in the modulearea.
@@ -1518,7 +1489,8 @@ static void __init map_lowmem(void)
 }
 
 #ifdef CONFIG_ARM_PV_FIXUP
-typedef void pgtables_remap(long long offset, unsigned long pgd);
+extern unsigned long __atags_pointer;
+typedef void pgtables_remap(long long offset, unsigned long pgd, void *bdata);
 pgtables_remap lpae_pgtables_remap_asm;
 
 /*
@@ -1531,6 +1503,7 @@ static void __init early_paging_init(const struct machine_desc *mdesc)
 	unsigned long pa_pgd;
 	unsigned int cr, ttbcr;
 	long long offset;
+	void *boot_data;
 
 	if (!mdesc->pv_fixup)
 		return;
@@ -1547,6 +1520,7 @@ static void __init early_paging_init(const struct machine_desc *mdesc)
 	 */
 	lpae_pgtables_remap = (pgtables_remap *)(unsigned long)__pa(lpae_pgtables_remap_asm);
 	pa_pgd = __pa(swapper_pg_dir);
+	boot_data = __va(__atags_pointer);
 	barrier();
 
 	pr_info("Switching physical address space to 0x%08llx\n",
@@ -1582,7 +1556,7 @@ static void __init early_paging_init(const struct machine_desc *mdesc)
 	 * needs to be assembly.  It's fairly simple, as we're using the
 	 * temporary tables setup by the initial assembly code.
 	 */
-	lpae_pgtables_remap(offset, pa_pgd);
+	lpae_pgtables_remap(offset, pa_pgd, boot_data);
 
 	/* Re-enable the caches and cacheable TLB walks */
 	asm volatile("mcr p15, 0, %0, c2, c0, 2" : : "r" (ttbcr));
diff --git a/arch/arm/mm/pgd.c b/arch/arm/mm/pgd.c
index f8e9bc58a84f..c5e1b27046a8 100644
--- a/arch/arm/mm/pgd.c
+++ b/arch/arm/mm/pgd.c
@@ -66,21 +66,7 @@ pgd_t *pgd_alloc(struct mm_struct *mm)
 	new_pmd = pmd_alloc(mm, new_pud, 0);
 	if (!new_pmd)
 		goto no_pmd;
-#ifdef CONFIG_KASAN
-	/*
-	 * Copy PMD table for KASAN shadow mappings.
-	 */
-	init_pgd = pgd_offset_k(TASK_SIZE);
-	init_p4d = p4d_offset(init_pgd, TASK_SIZE);
-	init_pud = pud_offset(init_p4d, TASK_SIZE);
-	init_pmd = pmd_offset(init_pud, TASK_SIZE);
-	new_pmd = pmd_offset(new_pud, TASK_SIZE);
-	memcpy(new_pmd, init_pmd,
-	       (pmd_index(MODULES_VADDR) - pmd_index(TASK_SIZE))
-	       * sizeof(pmd_t));
-	clean_dcache_area(new_pmd, PTRS_PER_PMD * sizeof(pmd_t));
-#endif /* CONFIG_KASAN */
-#endif /* CONFIG_LPAE */
+#endif
 
 	if (!vectors_high()) {
 		/*
diff --git a/arch/arm/mm/pv-fixup-asm.S b/arch/arm/mm/pv-fixup-asm.S
index 5c5e1952000a..8eade0416739 100644
--- a/arch/arm/mm/pv-fixup-asm.S
+++ b/arch/arm/mm/pv-fixup-asm.S
@@ -39,8 +39,8 @@ ENTRY(lpae_pgtables_remap_asm)
 
 	/* Update level 2 entries for the boot data */
 	add	r7, r2, #0x1000
-	movw	r3, #FDT_FIXED_BASE >> (SECTION_SHIFT - L2_ORDER)
-	add	r7, r7, r3
+	add	r7, r7, r3, lsr #SECTION_SHIFT - L2_ORDER
+	bic	r7, r7, #(1 << L2_ORDER) - 1
 	ldrd	r4, r5, [r7]
 	adds	r4, r4, r0
 	adc	r5, r5, r1
diff --git a/arch/arm/vdso/Makefile b/arch/arm/vdso/Makefile
index b558bee0e1f6..150ce6e6a5d3 100644
--- a/arch/arm/vdso/Makefile
+++ b/arch/arm/vdso/Makefile
@@ -42,8 +42,6 @@ GCOV_PROFILE := n
 # Prevents link failures: __sanitizer_cov_trace_pc() is not linked in.
 KCOV_INSTRUMENT := n
 
-KASAN_SANITIZE := n
-
 # Force dependency
 $(obj)/vdso.o : $(obj)/vdso.so
 
-- 
2.29.2

