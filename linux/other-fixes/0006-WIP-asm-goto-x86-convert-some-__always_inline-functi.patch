From 53cbcd6bdca4b722dfaecd27dda351a77ee898c2 Mon Sep 17 00:00:00 2001
From: Nick Desaulniers <ndesaulniers@google.com>
Date: Thu, 6 Sep 2018 15:49:04 -0700
Subject: [PATCH 6/6] WIP: asm goto: x86: convert some __always_inline
 functions to macros

The use of the "i" constraint modifier in inline assembly with operands
of inline functions is problematic, as it is code that is only valid in
GCC at -O2 (but results in a compilation error at -O0).

Example: https://godbolt.org/z/ttM5Eg

"i":
An immediate integer operand (one with constant value) is allowed. This
includes symbolic constants whose values will be known only at assembly
time or later.
https://gcc.gnu.org/onlinedocs/gcc/Simple-Constraints.html#Simple-Constraints

The operands of a static inline function are NOT immediate integer
operands, unless they are inlined into a callsite whose arguments are
immediate integer operands.

That this even works, but only at -O2, takes advantage of the
implementation detail of GCC, in which semantic analysis is performed
somewhat during optimizations (at least after inlining).

The correct way to rely on the inline asm inputs to be compatible with
"i" is to convert the inline functions into macros; as the C
preprocessor is guaranteed to perform the inlining before any semantic
analysis is run. Further, to continue to be used as expressions, they
are converted to GNU statement expressions.  Finally, in order to
prevent multiple label definition errors (labels in C are function
scoped) in functions that call these new macros multiple times, we take
advantage of local labels, a GNU C extension.
---
 arch/x86/include/asm/cpufeature.h | 92 ++++++++++++++++---------------
 arch/x86/include/asm/jump_label.h | 70 ++++++++++++-----------
 include/linux/jump_label.h        | 12 ++--
 include/linux/perf_event.h        | 10 ++--
 4 files changed, 96 insertions(+), 88 deletions(-)

diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h
index ce95b8cbd229..82125ba04ace 100644
--- a/arch/x86/include/asm/cpufeature.h
+++ b/arch/x86/include/asm/cpufeature.h
@@ -159,49 +159,55 @@ extern void clear_cpu_cap(struct cpuinfo_x86 *c, unsigned int bit);
  * These will statically patch the target code for additional
  * performance.
  */
-static __always_inline __pure bool _static_cpu_has(u16 bit)
-{
-	asm_volatile_goto("1: jmp 6f\n"
-		 "2:\n"
-		 ".skip -(((5f-4f) - (2b-1b)) > 0) * "
-			 "((5f-4f) - (2b-1b)),0x90\n"
-		 "3:\n"
-		 ".section .altinstructions,\"a\"\n"
-		 " .long 1b - .\n"		/* src offset */
-		 " .long 4f - .\n"		/* repl offset */
-		 " .word %P[always]\n"		/* always replace */
-		 " .byte 3b - 1b\n"		/* src len */
-		 " .byte 5f - 4f\n"		/* repl len */
-		 " .byte 3b - 2b\n"		/* pad len */
-		 ".previous\n"
-		 ".section .altinstr_replacement,\"ax\"\n"
-		 "4: jmp %l[t_no]\n"
-		 "5:\n"
-		 ".previous\n"
-		 ".section .altinstructions,\"a\"\n"
-		 " .long 1b - .\n"		/* src offset */
-		 " .long 0\n"			/* no replacement */
-		 " .word %P[feature]\n"		/* feature bit */
-		 " .byte 3b - 1b\n"		/* src len */
-		 " .byte 0\n"			/* repl len */
-		 " .byte 0\n"			/* pad len */
-		 ".previous\n"
-		 ".section .altinstr_aux,\"ax\"\n"
-		 "6:\n"
-		 " testb %[bitnum],%[cap_byte]\n"
-		 " jnz %l[t_yes]\n"
-		 " jmp %l[t_no]\n"
-		 ".previous\n"
-		 : : [feature]  "i" (bit),
-		     [always]   "i" (X86_FEATURE_ALWAYS),
-		     [bitnum]   "i" (1 << (bit & 7)),
-		     [cap_byte] "m" (((const char *)boot_cpu_data.x86_capability)[bit >> 3])
-		 : : t_yes, t_no);
-t_yes:
-	return true;
-t_no:
-	return false;
-}
+#define _static_cpu_has(bit) ({ \
+	__label__ t_yes; \
+	__label__ t_no; \
+	__label__ l_done; \
+	bool ret; \
+	asm_volatile_goto("1: jmp 6f\n" \
+		 "2:\n" \
+		 ".skip -(((5f-4f) - (2b-1b)) > 0) * " \
+			 "((5f-4f) - (2b-1b)),0x90\n" \
+		 "3:\n" \
+		 ".section .altinstructions,\"a\"\n" \
+		 " .long 1b - .\n"		/* src offset */ \
+		 " .long 4f - .\n"		/* repl offset */ \
+		 " .word %P[always]\n"		/* always replace */ \
+		 " .byte 3b - 1b\n"		/* src len */ \
+		 " .byte 5f - 4f\n"		/* repl len */ \
+		 " .byte 3b - 2b\n"		/* pad len */ \
+		 ".previous\n" \
+		 ".section .altinstr_replacement,\"ax\"\n" \
+		 "4: jmp %l[t_no]\n" \
+		 "5:\n" \
+		 ".previous\n" \
+		 ".section .altinstructions,\"a\"\n" \
+		 " .long 1b - .\n"		/* src offset */ \
+		 " .long 0\n"			/* no replacement */ \
+		 " .word %P[feature]\n"		/* feature bit */ \
+		 " .byte 3b - 1b\n"		/* src len */ \
+		 " .byte 0\n"			/* repl len */ \
+		 " .byte 0\n"			/* pad len */ \
+		 ".previous\n" \
+		 ".section .altinstr_aux,\"ax\"\n" \
+		 "6:\n" \
+		 " testb %[bitnum],%[cap_byte]\n" \
+		 " jnz %l[t_yes]\n" \
+		 " jmp %l[t_no]\n" \
+		 ".previous\n" \
+		 : : [feature]  "i" (bit), \
+		     [always]   "i" (X86_FEATURE_ALWAYS), \
+		     [bitnum]   "i" (1 << (bit & 7)), \
+		     [cap_byte] "m" (((const char *)boot_cpu_data.x86_capability)[bit >> 3]) \
+		 : : t_yes, t_no); \
+t_yes: \
+	ret = true; \
+	goto l_done; \
+t_no: \
+	ret = false; \
+l_done: \
+  ret; \
+})
 
 #define static_cpu_has(bit)					\
 (								\
diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h
index 65191ce8e1cf..70c093c35bbd 100644
--- a/arch/x86/include/asm/jump_label.h
+++ b/arch/x86/include/asm/jump_label.h
@@ -18,38 +18,46 @@
 #include <linux/stringify.h>
 #include <linux/types.h>
 
-static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
-{
-	asm_volatile_goto("1:"
-		".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t"
-		".pushsection __jump_table,  \"aw\" \n\t"
-		_ASM_ALIGN "\n\t"
-		".long 1b - ., %l[l_yes] - . \n\t"
-		_ASM_PTR "%c0 + %c1 - .\n\t"
-		".popsection \n\t"
-		: :  "i" (key), "i" (branch) : : l_yes);
+#define arch_static_branch(key, branch) ({ \
+	__label__ l_yes; \
+	__label__ l_done; \
+	bool ret = false; \
+	asm_volatile_goto("1:" \
+		".byte " __stringify(STATIC_KEY_INIT_NOP) "\n\t" \
+		".pushsection __jump_table,  \"aw\" \n\t" \
+		_ASM_ALIGN "\n\t" \
+		".long 1b - ., %l[l_yes] - . \n\t" \
+		_ASM_PTR "%c0 + %c1 - .\n\t" \
+		".popsection \n\t"\
+		: :  "i" (key), "i" (branch) : : l_yes); \
+\
+	goto l_done; \
+l_yes: \
+	ret = true; \
+l_done: \
+	ret; \
+})
 
-	return false;
-l_yes:
-	return true;
-}
-
-static __always_inline bool arch_static_branch_jump(struct static_key *key, bool branch)
-{
-	asm_volatile_goto("1:"
-		".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t"
-		"2:\n\t"
-		".pushsection __jump_table,  \"aw\" \n\t"
-		_ASM_ALIGN "\n\t"
-		".long 1b - ., %l[l_yes] - . \n\t"
-		_ASM_PTR "%c0 + %c1 - .\n\t"
-		".popsection \n\t"
-		: :  "i" (key), "i" (branch) : : l_yes);
-
-	return false;
-l_yes:
-	return true;
-}
+#define arch_static_branch_jump(key, branch) ({ \
+	__label__ l_yes; \
+	__label__ l_done; \
+	bool ret = false; \
+	asm_volatile_goto("1:" \
+		".byte 0xe9\n\t .long %l[l_yes] - 2f\n\t" \
+		"2:\n\t" \
+		".pushsection __jump_table,  \"aw\" \n\t" \
+		_ASM_ALIGN "\n\t" \
+		".long 1b - ., %l[l_yes] - . \n\t" \
+		_ASM_PTR "%c0 + %c1 - .\n\t" \
+		".popsection \n\t" \
+		: :  "i" (key), "i" (branch) : : l_yes); \
+\
+	goto l_done; \
+l_yes: \
+	ret = true; \
+l_done: \
+	ret; \
+})
 
 #else	/* __ASSEMBLY__ */
 
diff --git a/include/linux/jump_label.h b/include/linux/jump_label.h
index 3e113a1fa0f1..a7666ff328d1 100644
--- a/include/linux/jump_label.h
+++ b/include/linux/jump_label.h
@@ -195,15 +195,11 @@ struct module;
 #define JUMP_TYPE_LINKED	2UL
 #define JUMP_TYPE_MASK		3UL
 
-static __always_inline bool static_key_false(struct static_key *key)
-{
-	return arch_static_branch(key, false);
-}
+#define static_key_false(key) ({ \
+	arch_static_branch(key, false); })
 
-static __always_inline bool static_key_true(struct static_key *key)
-{
-	return !arch_static_branch(key, true);
-}
+#define static_key_true(key) ({ \
+	!arch_static_branch(key, true); })
 
 extern struct jump_entry __start___jump_table[];
 extern struct jump_entry __stop___jump_table[];
diff --git a/include/linux/perf_event.h b/include/linux/perf_event.h
index 1d5c551a5add..f9d4ac2102f4 100644
--- a/include/linux/perf_event.h
+++ b/include/linux/perf_event.h
@@ -1047,12 +1047,10 @@ static inline void perf_fetch_caller_regs(struct pt_regs *regs)
 	perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
 }
 
-static __always_inline void
-perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
-{
-	if (static_key_false(&perf_swevent_enabled[event_id]))
-		__perf_sw_event(event_id, nr, regs, addr);
-}
+#define perf_sw_event(event_id, nr, regs, addr) ({ \
+	if (static_key_false(&perf_swevent_enabled[event_id])) \
+		__perf_sw_event(event_id, nr, regs, addr); \
+})
 
 DECLARE_PER_CPU(struct pt_regs, __perf_regs[4]);
 
-- 
2.20.1

